 
\documentclass[sigconf,authordraft]{acmart}


%\documentclass[sigconf]{acmart}
%\documentclass[manuscript, screen, review, anonymous]{acmart}

\usepackage{subcaption} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}


\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

 
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{10.1145/XXXXXXX.XXXXXXX}




%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ACM PODS '25]{2025 International Conference on Management of Data}{June 22--27, 2025}{Berlin, Germany}

\acmISBN{ }

\acmSubmissionID{ }

\begin{document}


\title[Spreadsheet Data Extractor (SDE)]{Spreadsheet Data Extractor (SDE): A Performance-Optimized, User-Centric Tool for Transforming Semi-Structured Excel Spreadsheets into Relational Data}

\author{Alexander Aue-Johr}
 \email{alexander.aue@thuenen.de}
\orcid{0009-0001-8683-3630}
\affiliation{%
  \institution{Thünen Institute for Rural Areas}
  \city{Brunswick}
  \country{Germany}
}

\author{Hardy Pundt}
 \email{hpundt@hs-harz.de}
\orcid{https://orcid.org/0000-0001-6985-5929 }
\affiliation{%
  \institution{Harz University of Applied Sciences}
  \city{Wernigerode}
  \country{Germany}
}
 
\begin{abstract}
  Spreadsheets are ubiquitous tools used across various domains. Despite their widespread use, analyzing and utilizing data stored in spreadsheets poses significant challenges due to their semi-structured nature. Data in spreadsheets are often formatted primarily for human readability, employing layouts and styles that are easily understood by people but are difficult for automated systems to interpret. While these unstructured formats offer advantages—such as providing an easily comprehensible hierarchy of metadata—they complicate automated data extraction. This paper introduces the Spreadsheet Data Extractor (SDE), an open-source tool designed to convert semi-structured spreadsheet data into structured formats without requiring programming knowledge. Building upon previous work, we have enhanced the SDE with incremental loading of worksheets, accurate rendering of cell dimensions by directly parsing the Excel file's XML content, and performance optimizations to handle large datasets efficiently. We compare our tool with existing solutions and demonstrate its effectiveness through performance evaluations, highlighting its potential to facilitate efficient and reliable data extraction from diverse spreadsheet formats.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010405.10010476.10011187.10011189</concept_id>
<concept_desc>Applied computing~Spreadsheets</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10002951.10002952.10003219.10003218</concept_id>
<concept_desc>Information systems~Data cleaning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011050.10010512.10003310</concept_id>
<concept_desc>Software and its engineering~Extensible Markup Language (XML)</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003124.10010865</concept_id>
<concept_desc>Human-centered computing~Graphical user interfaces</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003809.10010031.10002975</concept_id>
<concept_desc>Theory of computation~Data compression</concept_desc>
<concept_significance>100</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Spreadsheets}
\ccsdesc[500]{Information systems~Data cleaning}
\ccsdesc[500]{Software and its engineering~Extensible Markup Language (XML)}
\ccsdesc[300]{Human-centered computing~Graphical user interfaces}
\ccsdesc[100]{Theory of computation~Data compression}


\keywords{Spreadsheets, Data cleaning, Relational Data, Excel, XML, Graphical user interfaces}
\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

\maketitle
\section{Introduction}

Spreadsheets are ubiquitous tools used across various domains, including healthcare \cite{berndt2001healthcare}, nonprofit organizations \cite{singh2009numeric,west2008because}, finance, commerce, academia, and government \cite{dunn2010spreadsheets}. Despite their widespread use, analyzing and utilizing data stored in spreadsheets poses significant challenges for data analysis and utilization, primarily due to their semi-structured nature. Data within spreadsheets are frequently organized for human readability~\cite{tang2023efficient}, featuring layouts with empty cells, merged cells, hierarchical headers, and multiple tables. While these formats enhance comprehensibility for users~\cite{rahman2021noah}, they impede machine readability and automated data processing. This semi-structured organization complicates the extraction of meaningful insights, especially when attempting to integrate spreadsheet data into more robust and scalable data management systems.

The reliance on spreadsheets as ad-hoc solutions poses several limitations:
\begin{itemize}
    \item \textbf{Data Integrity and Consistency:} Spreadsheets are prone to errors, such as duplicate entries, inconsistent data formats, and inadvertent modifications, which can compromise data integrity.~\cite{cunha2012smellsheet, abreu2014faultysheet}
    \item \textbf{Scalability Issues:} As datasets grow in size and complexity, spreadsheets become less efficient for data storage and retrieval, leading to performance bottlenecks.~\cite{swidan2016improving, mack2018characterizing}
    \item \textbf{Limited Query Capabilities:} Unlike databases, spreadsheets lack advanced querying and indexing features, restricting users from performing complex data analyses.
    %TODO: \item \textbf{Integration Challenges:} Integrating spreadsheet data with other systems, applications, or databases is often cumbersome and error-prone, limiting interoperability. 
\end{itemize}

Transitioning from these ad-hoc spreadsheet solutions to standardized database systems offers numerous benefits:
\begin{itemize}
    \item \textbf{Enhanced Data Integrity:} Databases enforce data validation rules and constraints, ensuring higher data quality and consistency.
    \item \textbf{Improved Scalability:} Databases are designed to handle large volumes of data efficiently, supporting complex queries and transactions without significant performance degradation.
    \item \textbf{Advanced Querying and Reporting:} Databases provide powerful querying languages like SQL, enabling sophisticated data analysis and reporting capabilities.
    \item \textbf{Seamless Integration:} Databases facilitate easier integration with various applications and services, promoting interoperability and data sharing across platforms.
\end{itemize}

Given the abundance of existing spreadsheet data and the clear advantages of database systems, there is a pressing need for tools that can bridge the gap between these two formats. Automated and accurate data extraction from spreadsheets into relational database formats is essential for organizations to leverage their data assets effectively.

Previous work by Aue et al. introduced a tool that facilitates data extraction from Excel files~\cite{alexander2024converting}. While effective, their solution faced performance issues and inaccuracies in rendering cell dimensions, limiting its usability with large and complex datasets.

In this paper, we aim to transform semi-structured spreadsheet data into machine-readable formats by building upon their work. We present the Spreadsheet Data Extractor (SDE), an enhanced tool that enables users to define data hierarchies through cell selection without any programming knowledge.~\cite{spreadsheet_data_extractor} Our enhancements address key limitations of the existing solution, making data extraction from complex spreadsheets more efficient and user-friendly.

\subsection{Contributions}

Our main contributions are as follows:

\begin{enumerate}
    \item We release the SDE under the open-source GNU General Public License v3.0, promoting community access and collaboration~\cite{spreadsheet_data_extractor}.
    \item We implement incremental loading of worksheets to enhance performance, allowing the tool to handle large Excel files efficiently and even outperforming Microsoft excel in terms of loading speed, opening worksheets within milliseconds were excel takes seconds to minutes to show the first sheet
    \item We accurately render row heights and column widths by parsing the XML data contained in the Excel .xlsx archive files, ensuring that the spreadsheet's visual representation closely matches that of Excel.
    \item We do not rely on third party libraries to parse the excel files, because such libraries often use dom parsing which is very memory inefficient, that is why we implemented our own parser which extracts the sheet information solely by regular expressions, greatly reducing memory consumption and improving performance.
    \item We optimize the rendering engine to draw only the visible cells, significantly improving performance when dealing with large datasets.
    \item We integrate the selection hierarchy, worksheet view, and output preview into a unified interface, streamlining the user experience.
\end{enumerate}
  

\section{Related Work}


\begin{table*}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
      \hline
      \textbf{Name} & \textbf{Technologies} & \textbf{Output} & \textbf{Accessibility} & \textbf{Frequency} \\ \hline
      DeExcelerator & heuristics            & cleaned data    & partially open source  & last publication   \\
                    &                       &                 &                & 2015               \\ \hline
      FlashRelate   & AI               & cleaned data    & proprietary,           & last publication   \\
                    & programming-by-example &                 & no access              & 2015               \\ \hline


      Senbazuru     & AI               & cleaned data    & partially open source  & last commit        \\
                    &                       &                 &                        & 2015               \\\hline

      XLindy        & AI                    & cleaned data    & no access              & cancelled          \\ \hline
      TableSense    & AI                    & diagrams        & proprietary,           & last commit        \\
                    &                       &                 & no access              & 2021               \\ \hline
  \end{tabular}
  \caption{Spreadsheet Data Extractor counterparts}

  \label{tab:approaches}
\end{table*}

The extraction of relational data from semi-structured documents, particularly spreadsheets, has garnered significant attention due to their ubiquitous use across domains such as business, government, and scientific research. Several frameworks and tools have been developed to address the challenges of converting flexible spreadsheet formats into normalized relational forms suitable for data analysis and integration. Notable among these are \textbf{DeExcelerator}~\cite{eberius2013deexcelerator} , \textbf{XLIndy}~\cite{koci2019xlindy}, \textbf{FLASHRELATE}~\cite{barowy2015flashrelate}, \textbf{Senbazuru}~\cite{chen2013senbazuru}, \textbf{TableSense}~\cite{dong2019tablesense}, and the approach by Aue et al.~\cite{alexander2024converting}, on which our work builds.

\subsection{Aue et al.'s Converter}

Aue et al.~\cite{alexander2024converting} developed a tool to facilitate data extraction from Excel spreadsheets by leveraging the Dart excel package~\cite{excelPackage} to process .xlsx files. This tool allows users to define data hierarchies by selecting relevant cells containing data and metadata. However, the approach faced significant performance bottlenecks due to the excel package's requirement to load the entire .xlsx file into memory, resulting in slow response times, particularly for large files.

In addition to memory issues, the tool calculated row heights and column widths based solely on cell content, ignoring the dimensions specified in the original Excel file. This led to rendering discrepancies between the tool and the original spreadsheet. Furthermore, the tool rendered all cells, regardless of their visibility within the viewport, significantly degrading performance when handling worksheets with large numbers of cells.

\subsection{DeExcelerator}

Eberius et al.~\cite{eberius2013deexcelerator} introduced \textbf{DeExcelerator}, a framework that transforms partially structured spreadsheets into first normal form relational tables using heuristic-based extraction phases. It addresses challenges such as table detection, metadata extraction, and layout normalization. While effective in automating normalization, its reliance on predefined heuristics limits adaptability to heterogeneous or unconventional spreadsheet formats, highlighting the need for more flexible approaches.

\subsection{XLIndy}

Koci et al.~\cite{koci2019xlindy} developed \textbf{XLIndy}, an interactive Excel add-in with a Python-based machine learning backend. Unlike DeExcelerator’s fully automated heuristic approach, XLIndy integrates machine learning techniques for layout inference and table recognition, enabling a more adaptable and accurate extraction process. XLIndy's interactive interface allows users to visually inspect extraction results, adjust configurations, and compare different extraction runs, facilitating iterative fine-tuning. Additionally, users can manually revise predicted layouts and tables, saving these revisions as annotations to improve classifier performance through (re-)training. This user-centric approach enhances the tool’s flexibility, allowing it to accommodate diverse spreadsheet formats and user-specific requirements more effectively than purely heuristic-based systems.

\subsection{FLASHRELATE}

Barowy et al.~\cite{barowy2015flashrelate} presented \textbf{FLASHRELATE}, an approach that empowers users to extract structured relational data from semi-structured spreadsheets without requiring programming expertise. FLASHRELATE introduces a domain-specific language, \textbf{FLARE}, which extends traditional regular expressions with spatial constraints to capture the geometric relationships inherent in spreadsheet layouts. Additionally, FLASHRELATE employs an algorithm that synthesizes FLARE programs from a small number of user-provided positive and negative examples, significantly simplifying the automated data extraction process.

FLASHRELATE distinguishes itself from both DeExcelerator and XLIndy by leveraging programming-by-example (PBE) techniques. While DeExcelerator relies on predefined heuristic rules and XLIndy incorporates machine learning models requiring user interaction for fine-tuning, FLASHRELATE allows non-expert users to define extraction patterns through intuitive examples. This approach lowers the barrier to entry for extracting relational data from complex spreadsheet encodings, making the tool accessible to a broader range of users.

\subsection{Senbazuru}

Chen et al.~\cite{chen2013senbazuru} introduced \textbf{Senbazuru}, a prototype Spreadsheet Database Management System (SSDBMS) designed to extract relational information from a large corpus of spreadsheets. Senbazuru addresses the critical issue of integrating data across multiple spreadsheets, which often lack explicit relational metadata, thereby hindering the use of traditional relational tools for data integration and analysis.

Senbazuru comprises three primary functional components:

\begin{enumerate}
    \item \textbf{Search}: Utilizing a textual search-and-rank interface, Senbazuru enables users to quickly locate relevant spreadsheets within a vast corpus. The search component indexes spreadsheets using Apache Lucene, allowing for efficient retrieval based on relevance to user queries.
    
    \item \textbf{Extract}: The extraction pipeline in Senbazuru consists of several stages:
    \begin{itemize}
        \item \textbf{Frame Finder}: Identifies data frame structures within spreadsheets using Conditional Random Fields (CRFs) to assign semantic labels to non-empty rows, effectively detecting rectangular value regions and associated attribute regions.
        \item \textbf{Hierarchy Extractor}: Recovers attribute hierarchies for both left and top attribute regions. This stage also incorporates a user-interactive repair interface, allowing users to manually correct extraction errors, which the system then generalizes to similar instances using probabilistic methods.
        \item \textbf{Tuple Builder and Relation Constructor}: Generates relational tuples from the extracted data frames and assembles these tuples into coherent relational tables by clustering attributes and recovering column labels using external schema repositories like Freebase and YAGO.
    \end{itemize}
    
    \item \textbf{Query}: Supports basic relational operations such as selection and join on the extracted relational tables, enabling users to perform complex data analysis tasks without needing to write SQL queries.
\end{enumerate}

Senbazuru's ability to handle hierarchical spreadsheets, where attributes may span multiple rows or columns without explicit labeling, sets it apart from earlier systems like DeExcelerator and XLIndy. By employing machine learning techniques and providing user-friendly repair interfaces, Senbazuru ensures high-quality extraction and facilitates the integration of spreadsheet data into relational databases.


\subsection{TableSense}

Dong et al.~\cite{dong2019tablesense} developed \textbf{TableSense}, an end-to-end framework for spreadsheet table detection using Convolutional Neural Networks (CNNs). TableSense addresses the diversity of table structures and layouts by introducing a comprehensive cell featurization scheme, a Precise Bounding Box Regression (PBR) module for accurate boundary detection, and an active learning framework to efficiently build a robust training dataset.

While \textbf{DeExcelerator}, \textbf{XLIndy}, \textbf{FLASHRELATE}, and \textbf{Senbazuru} focus primarily on transforming spreadsheet data into relational forms through heuristic, machine learning, and programming-by-example approaches, \textbf{TableSense} specifically targets the accurate detection of table boundaries within spreadsheets using deep learning techniques. Unlike region-growth-based methods employed in commodity spreadsheet tools, which often fail on complex table layouts, TableSense achieves superior precision and recall by leveraging CNNs tailored for the unique characteristics of spreadsheet data.
However, TableSense focuses on table detection and visualization, allowing users to generate diagrams from the detected tables but does not provide functionality for exporting the extracted data for further analysis.


\subsection{Comparison and Positioning}

While \textbf{DeExcelerator}, \textbf{XLIndy}, \textbf{FLASHRELATE}, \textbf{Senbazuru}, and \textbf{TableSense} each offer unique approaches to spreadsheet data extraction, they share certain limitations. Many of these tools are not readily accessible: \textbf{FLASHRELATE} and \textbf{TableSense} are proprietary, and \textbf{Senbazuru}, \textbf{XLIndy}, and \textbf{DeExcelerator} are discontinued projects with limited or no source code availability. In contrast, we contribute our spreadsheet data extractor under the GNU General Public License v3.0, allowing the community to access, use, and improve the tool freely.

Moreover, unlike the aforementioned tools that rely on heuristics, machine learning, or AI techniques—which can introduce errors requiring users to identify and correct—we adopt a user-centric approach that gives users full control over data selection and metadata hierarchy definition. While this requires more manual input, it eliminates the uncertainty and potential inaccuracies associated with automated methods. To streamline the process and enhance efficiency, our tool includes user-friendly features such as the ability to duplicate hierarchies of columns and tables, and to move them over similar structures for reuse, reducing the need for repetitive configurations.



By combining the strengths of manual control with enhanced user interface features and performance optimizations, our tool offers a robust and accessible solution for extracting relational data from complex and visually intricate spreadsheets. These enhancements not only improve performance and accuracy but also elevate the overall user experience, making our tool a valuable asset for efficient and reliable data extraction from diverse spreadsheet formats.

 




\section{Methodology}

\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{images/spreadsheet_data_extractor/new_version/nothing.png}
  \caption{The SDE Interface Overview.}
  \label{fig:nothing}
\end{figure*}%



In this section, we detail the design and implementation of the Spreadsheet Data Extractor (SDE), emphasizing its user-centric approach and performance optimizations. The SDE enables users to transform semi-structured spreadsheet data into structured, machine-readable formats without requiring programming expertise. We achieve this through an intuitive interface that allows for cell selection and hierarchy definition, incremental loading of worksheets, accurate rendering of cell dimensions, and optimized performance for handling large datasets by incrementally loading worksheets and by rendering only the cells that are currently visible in the view.

\subsection{User-Centric Data Extraction}

The core functionality of the SDE revolves around enabling users to select cells containing data and metadata to define a data hierarchy. This process is facilitated through a graphical interface that displays the spreadsheet and supports intuitive selection and manipulation of the selection hierarchy.

\subsubsection{Hierarchy Definition}

Users can select individual cells or ranges of cells by clicking and using shift-click for multi-selection. These selections represent either data or metadata. The selected cells are organised into a hierarchical tree structure, where each node represents a data element, and child nodes represent nested data or metadata. This hierarchy defines how the data will be transformed into a structured format.

The interface further supports flexible hierarchy management by allowing users to drag and drop hierarchy nodes onto other nodes to realign the structure whenever a different organisation is deemed more appropriate.

In addition, users can create custom nodes with user-defined text, enabling them to include metadata that is implicit in the spreadsheet but absent as explicit cell content. 

\subsubsection{Reusability and Efficiency}

To optimise the extraction process and reduce repetitive tasks, the SDE allows users to duplicate previously defined hierarchies and apply them to similar regions within the spreadsheet. This feature is particularly useful for spreadsheets with repeating structures, such as multiple tables with the same format. 

When duplicating and moving a cell selection hierarchy to another section of the spreadsheet, it is sometimes necessary to keep certain cells fixed while moving the rest. This scenario commonly occurs when multiple tables are vertically aligned, and the tables below do not repeat their header rows. To address this, the SDE provides a lock function while the \texttt{DuplicateAndMove} mode is active. It allows users to freeze specific cells in place while relocating the hierarchy. Cells can be locked by clicking on the lock symbol at the top-left corner of the cell or to the right of the corresponding selection in the hierarchy panel. Figure~\ref{fig:5_repetitions} illustrates this. Already locked selections are displayed accordingly and can be unlocked if needed. Locked cells remain stationary, whereas the other cells in the hierarchy are repositioned accordingly. 

\subsection{Example Workflow}


Consider a spreadsheet containing statistical forecasts of future nursing staff availability in Germany \cite{destatis2024pflegekraefte}. Figure~\ref{fig:nothing} shows the SDE interface, which consists of three main components:


\textbf{Hierarchy Panel (Top Left):} Displays the hierarchy of cell selections, initially empty.

\textbf{Spreadsheet View (Top Right):} Shows the currently opened Excel file and the currently selected worksheet for cell selection.

\textbf{Output Preview (Bottom):} Provides immediate feedback on the data extraction based on current selections.



 
 

\subsubsection{Selection of the First Column} 

The user adds a node to the hierarchy and selects the cell containing the metadata "Nursing Staff" (Figure~\ref{fig:first_column}). This cell represents metadata that is common to all cells in this worksheet. Therefore, it should be selected first and should appear at the beginning of each row in the output CSV file.


\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/first_column.png}
  \caption{Selection of the First Column Metadata}
  \label{fig:first_column}
\end{figure}%

 
 

Within this node, the user adds a child node and selects the cell \emph{"Total"}, which serves as both a table header and a row label. This selection represents the table header of the first subtable. The user adds another child node and selects the range of cells containing row labels (e.g., \emph{"Total", "15-20", "20-25"} and so forth) by clicking the first cell and shift-clicking the last cell.

A further child node is then placed under the row labels node, and the user selects the year \emph{"2024"}. Subsequently, an additional child node is created beneath the year node, and the user selects the corresponding data cells (e.g., \emph{"1673", "53", "154"}, etc.).

At this point, the hierarchy consists of five nodes, each—except the last one—containing an embedded child node. In the upper-right portion of the interface, the chosen cells are displayed in distinct colors corresponding to each node. The lower area shows a preview of the extracted output. For each child node, an additional column is appended to the output. When multiple cells are selected for a given node, their values appear as entries in new rows of the output, reflecting the defined hierarchical structure.

\subsubsection{Duplicating the Column Hierarchy}

 
To avoid repetitive manual entry for additional years, the user duplicates the hierarchy for "2024" and adjusts the cell selections to include data for subsequent years (e.g., "2025," "2026") using the "Move and Duplicate" feature.


\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/move_and_duplicate_2024_half.png}
  \caption{Invoking the "move and duplicate" feature on the 2024 column node.}
  \label{fig:move_and_duplicate_2024}
\end{figure}

To do this, the user selects the node of the first column "2024" and right-clicks on it. A popup opens in which the action "move and duplicate" appears, which should then be clicked, as shown in Figure \ref{fig:move_and_duplicate_2024}.



\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/move_and_duplicate/duplicate_columns_lock_symbol.png}
  \caption{Moving the hierarchy one cell to the right while adjusting the number of repetitions to duplicate the column selection.}
  \label{fig:5_repetitions}
\end{figure}

Subsequently, a series of buttons opens in the app bar at the top right, allowing the user to move the cell selections of the node as well as all child nodes, as seen in Figure \ref{fig:5_repetitions}. By pressing the button to move the selection by one unit to the right, the next column is selected. However, this would also deselect the first column since the selection was moved. To preserve the first column, the "move and duplicate" checkbox can be activated. This creates the shifted selection in addition to the original selection. The changes are only applied when the "accept" button is clicked. The next columns could also be selected in the same way. But this can be done faster, because instead of moving the selection and duplicating it only once, the "repeat" input field can be filled with as many repetitions as there are columns. By entering the number 5, the selection of the first column is shifted 5 times by one unit to the right and duplicated at each step.

The user reviews the selections in the spreadsheet view, where each selection is highlighted in a different color corresponding to its node in the hierarchy. Only after the user has reviewed the shifted and duplicated selections in the worksheet and clicked the "accept" button are the nodes in the hierarchy created as desired. Figure \ref{fig:move_accept} shows the resulting selection after the user approved the

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/move_accept.png}
  \caption{Resulting Hierarchy After Move and Accept}
  \label{fig:move_accept}
\end{figure}%





 




\subsubsection{Duplicating the Table Hierarchy}

The same method that worked effectively for duplicating the columns can now be applied to the subtables, as shown in Figure~\ref{fig:all_cells_selected}.

\begin{figure*}[t] 
  \centering 
  \includegraphics[width=\textwidth]{images/spreadsheet_data_extractor/new_version/move_and_duplicate/duplicate_tables.png} 
  \caption{Selection of All Cells in the Subtables by Duplicating the Hierarchy of the First Table} 
  \label{fig:all_cells_selected} 
\end{figure*}

By selecting the node with the value "Total" and clicking the "Move and Duplicate" button, we can apply the selection of the "Total" subtable to the other subtables. This involves shifting the table downward by as many rows as necessary to overlap with the subtable below.

However, there is a minor issue: the child nodes of the "Total" node also include the column headers. If these column headers were repeated in the subtables below, shifting the selections downward would work without modification. Since these cells are not repeated in the subtables, we need to prevent the column headers cells from moving during the duplication process.

To achieve this, we can exclude individual nodes from being moved by locking their selection. This is done by clicking the padlock icon on the corresponding nodes, which freezes their cell selection and keeps them fixed at their original position, regardless of other cells being moved.

Therefore, we identify and select the nodes containing the column headers—specifically, the years 2024 to 2049—and lock their selection using the padlock button. By shifting the selection downward and duplicating it, we can easily move and duplicate the cell selections for the subtables below. By setting the number of repetitions to 2, all subtables are completely selected.


\subsection{Cross Product Transformation}

The graph resulting from the selected hierarchy is shown in Figure~\ref{fig:crossBefore}. To simplify the explanation, the example is limited to the first three columns and the first three rows of the first sub-table.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/cross_before.pdf}
  \caption{Illustration of the Cross Product Transformation Before Application}
  \label{fig:crossBefore}
\end{figure}%


Once the hierarchy is defined, the \textbf{Structured Data Engine (SDE)} applies a cross-product transformation to generate a relational format from the selection graph. This transformation consists of two key steps:

\begin{enumerate}
    \item \textbf{Node Duplication:}  
    Nodes with multiple incoming or outgoing edges (e.g., the row labels node with the values \textit{Total}, \textit{15-20}, \textit{20-25}) are duplicated to ensure that each edge connects to a unique instance of the node. This adjustment replaces the original many-to-one relationships with one-to-one mappings for each edge.
    
    \item \textbf{Value Replication:}  
    Nodes containing single values (e.g., the year \textit{2024}) are replicated to align with the number of values associated with the connected node. This ensures a consistent structure in the relational output, maintaining alignment across all hierarchical levels. 
\end{enumerate}

The resulting graph after the cross-product transformation is shown in Figure~\ref{fig:cross_after}. With this transformation applied to the selected hierarchy, the SDE generates a structured output that reflects the original spreadsheet's hierarchical relationships, enabling users to analyze and integrate the data effectively.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/cross_after.pdf}
  \caption{Illustration of the Cross Product Transformation After Application}
  \label{fig:cross_after}
\end{figure}%
 











 

\subsection{Incremental Loading of Worksheets}

Opening large Excel files traditionally involves loading the entire file and all its worksheets into memory before displaying any content. In files containing very large worksheets, this process can take several seconds to minutes, causing significant delays for users who need to access data quickly.

To facilitate efficient data extraction from multiple Excel files, we implemented a mechanism for incremental loading of worksheets within the SDE. Excel files (\texttt{.xlsx} format) are ZIP archives containing a collection of XML files that describe the worksheets, styles, and shared strings. Key components include:

\begin{itemize}
    \item \textbf{\texttt{xl/sharedStrings.xml}}: Contains all the unique strings used across worksheets, reducing redundancy.
    \item \textbf{\texttt{xl/styles.xml}}: Defines the formatting styles for cells, including fonts, colors, and borders.
    \item \textbf{\texttt{xl/worksheets/sheetX.xml}}: Represents individual worksheets (\texttt{sheet1.xml}, \texttt{sheet2.xml}, etc.).
\end{itemize}

Our solution opens the Excel file as a ZIP archive and initially extracts only the essential metadata and shared resources required for the application to function. This initial extraction includes:

\begin{enumerate}
    \item \textbf{Metadata Extraction}:

    We read the archive's directory to identify the contained files without decompressing them fully. This step is quick, taking only a few milliseconds, and provides information about the available worksheets and shared resources.

    \item \textbf{Selective Extraction}:

    We immediately extract \texttt{sharedStrings.xml} and \texttt{styles.xml} because these files are small and contain information necessary for rendering cell content and styles across all worksheets. These files are parsed and stored in memory for quick access during rendering.

    \item \textbf{Deferred Worksheet Loading}:

    The individual worksheet files (\texttt{sheetX.xml}) remain compressed and are loaded into memory in their binary unextracted form. They are not decompressed or parsed at this stage.

    \item \textbf{On-Demand Parsing}:

    When a user accesses a specific worksheet—either by selecting it in the interface or when a unit test requires data from it—the corresponding \texttt{sheetX.xml} file is then decompressed and parsed. This parsing occurs in the background and is triggered only by direct user action or programmatic access to the worksheet's data.

    \item \textbf{Memory Release}:

    After a worksheet has been decompressed and its XML parsed, we release the memory resources associated with the parsed data. This approach prevents excessive memory usage and ensures that the application remains responsive even when working with multiple large worksheets.
\end{enumerate}

By adopting this incremental loading approach, users experience minimal wait times when opening an Excel file. The initial loading is nearly instantaneous, allowing users to begin interacting with the application without delay. This contrasts with traditional methods that require loading all worksheets upfront, leading to significant wait times for large files.

 
\subsection{Parsing Worksheet XML with Regular Expressions}

In our previous attempts to parse Excel files using third-party libraries, we encountered persistent failures when processing very large files. Libraries such as the Dart \texttt{excel} package or Python tools like Pandas and OpenPyXL typically load Excel files by relying on a DOM parser or by identifying parts of the worksheet using regular expressions and subsequently parsing these fragments with an XML parser. In contrast, the \emph{Spreadsheet Data Extractor} relies solely on regular expressions to parse the entire contents of worksheets. Rows are first identified by splitting the file content at each \texttt{<row>} element. Each resulting fragment is then processed further.

For every such fragment, the content is split at each \texttt{<c>} element. The individual \texttt{<row>} and \texttt{<c>} elements are subsequently parsed using regular expressions containing multiple capture groups to extract the information required for rendering rows and cells. This approach enables the opening of files that suffer from severe file bloat---an issue that poses significant challenges for XML-based parsers. Such parsers attempt to load hundreds of thousands or even millions of cells that have no functional relevance for the visual representation or computational logic of the worksheet but were inadvertently added by users. Nevertheless, these cells are still parsed as individual XML elements and stored in memory to facilitate efficient traversal.

However, the size of these files frequently exceeds the capabilities of DOM-based parsers. An XML DOM tree typically requires approximately ten times the memory of the raw string representation. Consequently, a bloated worksheet that already expands to several gigabytes when decompressed can easily exceed the available memory on a consumer-grade laptop once represented as a DOM tree. In one extreme case, an Excel file provided by the German Federal Statistical Office could not be opened with Python and Pandas even on a test system equipped with 96\,GB of RAM and several hundred gigabytes of virtual memory on an NVMe M.2 SSD. After two days without success, the attempt was aborted. By contrast, the \emph{Spreadsheet Data Extractor} is able to open the same file within milliseconds because regular-expression-based parsing is substantially more memory- and performance-efficient than DOM-based approaches.

\subsubsection{Parsing of \texttt{<c>} Nodes}

The parsing of individual \texttt{<c>} (cell) nodes in the \emph{Spreadsheet Data Extractor} is performed using a single regular expression that captures all relevant attributes and values required for cell representation. The following pattern is applied:


This regular expression identifies every cell element by matching the opening \texttt{<c>} tag and its attributes. The key information is extracted via capture groups:

\begin{itemize}
  \item Groups 2 and 3 extract the column reference (letters) and row index (numbers) from the attribute \texttt{r}, enabling the mapping of a cell to its precise worksheet location.
  \item Group 4 captures the style index (\texttt{s}), which can be used to determine cell formatting.
  \item Group 5 extracts the cell type (\texttt{t}), for instance to distinguish between shared strings, booleans, or numbers.
  \item Groups 6 and 7 capture either the value enclosed in a \texttt{<v>} tag or an inline string enclosed in an \texttt{<is>} tag.
\end{itemize}

\begin{lstlisting}[caption={Regex for parsing cells},label={lst:regex_cells}]
<c\b'
(
  (?:[^>]*\br="([A-Z]+)(\d+)")? // Groups 2+3
  (?:[^>]*\bs="(\d+?)")?        // Group 4
  (?:[^>]*\bt="([^"]+)")?       // Group 5
  [^/>]*?
)
(?:/>
  |>
  (?:[^<]*(?:
    <v>(.*?)</v>                 // Group 6
    |
    <is>(.*?)</is>)              // Group 7
  )? 
</c>)
\end{lstlisting}
By using this approach, all relevant attributes and values of a cell are captured without requiring a DOM tree. This avoids the significant memory overhead that XML parsers incur when representing each cell as a distinct node. Instead, the required metadata and values are extracted directly from the raw XML string, allowing the \emph{Spreadsheet Data Extractor} to parse even extremely large worksheets efficiently in terms of both time and memory consumption.


\subsection{Rendering of Worksheets}

To ensure that users can navigate worksheets without difficulty, we prioritize displaying the worksheets in a manner that closely resembles their appearance in Excel. This involves accurately rendering cell dimensions, formatting, and text behaviors.

\subsubsection{Displaying Row Heights and Column Widths}

Our solution extracts information about column widths and row heights directly from the Excel file's XML structure. Specifically, we retrieve the column widths from the \textit{width} attribute of the \texttt{<col>} elements and the row heights from the \textit{ht} attribute of the \texttt{<row>} elements in the \texttt{sheetX.xml} files.

In Excel, column widths and row heights use units that do not directly map to pixels, requiring conversion for accurate on-screen rendering. Different scaling factors are needed for columns and rows. Through empirical testing, we derived the following scaling factors:

\begin{itemize} 
  \item \textbf{Column Widths}: Multiply the \textit{width} attribute by 7.
  \item \textbf{Row Heights}: Multiply the \textit{ht} attribute by \(\frac{4}{3}\).
\end{itemize}
Despite research, we could not find official documentation explaining the rationale behind these specific scaling factors. This lack of documentation poses a challenge for accurately replicating Excel’s rendering.

\subsubsection{Cell Formatting}

Cell formatting plays a crucial role in accurately representing the appearance of worksheets. Formatting information is stored in the \texttt{styles.xml} file, where styles are defined and later referenced in the \texttt{sheetX.xml} files as shown in Figure \ref{fig:formatting}.

\begin{figure}[h] 
  \centering 
  \includegraphics[width=\linewidth]{xml_minted/collage.pdf} 
  \caption{} 
  \label{fig:formatting} 
\end{figure}

Each cell in the worksheet references a style index through the \textit{s} attribute, which points to the corresponding \texttt{<xf>} element within the \texttt{cellXfs} collection. These \texttt{<xf>} elements contain attributes such as \textit{fontId}, \textit{fillId}, and \textit{borderId}, which reference specific font, fill (background), and border definitions located in the \texttt{fonts}, \texttt{fills}, and \texttt{borders} collections, respectively. By parsing these references, we can accurately apply the appropriate fonts, background colors, and border styles to each cell.

Through meticulous parsing and application of these formatting details, we ensure that the rendered worksheet closely mirrors the original Excel file, preserving the visual cues and aesthetics that users expect.

 

\subsubsection{Handling Text Overflow}

In Excel, when the content of a cell exceeds its width, the text may overflow into adjacent empty cells, provided those cells do not contain any data. If adjacent cells are occupied, Excel truncates the overflowing text at the cell boundary. Replicating this behavior is essential for accurate rendering and user familiarity.

We implemented text overflow handling by checking if the adjacent cell to the right is empty before allowing text to overflow. If the adjacent cell is empty, we extend the text rendering. If the adjacent cell contains data, we truncate the text at the boundary of the original cell.

Figure \ref{fig:nothing} illustrates this behavior. The text "Supply of Nursing Staff ..." extends into the neighboring cell because it is empty. If not for this handling, the text would be truncated at the cell boundary, leading to incomplete data display as shown in Figure \ref{fig:overflow}.

\begin{figure}[h] 
  \centering 
  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/overflow.png} 
  \caption{Falsche Darstellung ohne overflow von zellen mit angrenzenden zellen ohne inhalt} 
  \label{fig:overflow} 
\end{figure}

By accurately handling text overflow, we improve readability and maintain consistency with Excel's user interface, which is crucial for users transitioning between Excel and our tool.


\subsection{Performance Optimization}

To ensure high frame rates even with large worksheets, we optimized the Spreadsheet Data Extractor to render only the cells that are currently visible to the user. Rendering the entire worksheet, especially when it contains thousands of cells, can significantly degrade performance. By focusing on the visible cells within the viewport, we reduce computational overhead and improve responsiveness.

We achieve this optimization by utilizing the \textit{two\allowbreak\_dimensional\allowbreak\_scrollables} package \cite{flutter_packages_2024}. This package provides functionality for efficiently handling two-dimensional scrolling regions, making it suitable for rendering large grids like spreadsheets.

Since we extract all column widths and row heights from the XML files, we can calculate the exact dimensions and positions of each cell. By accumulating the widths and heights, we determine the coordinates of each cell within the worksheet grid. These coordinates are essential for identifying which cells fall within the current viewport and should be rendered.

We consider the current scroll offsets along the horizontal ($x$-axis) and vertical ($y$-axis) directions. The viewport is defined by the width and height of the panel displaying the Excel worksheet. To determine the visible cells, we perform the following steps:

\begin{itemize}
    \item \textbf{Horizontal Visibility}: 
    \begin{itemize}
        \item Sum the column widths until the accumulated sum reaches the left edge of the viewport. All cells to the left are ignored.
        \item Continue adding column widths until the sum exceeds the right edge of the viewport. Cells beyond this point are also ignored.
    \end{itemize}
    \item \textbf{Vertical Visibility}: 
    \begin{itemize}
        \item Sum the row heights until the accumulated sum reaches the top edge of the viewport. All cells above are ignored.
        \item Continue adding row heights until the sum surpasses the bottom edge of the viewport. Cells below this point are ignored.
    \end{itemize}
\end{itemize}

By rendering only the cells within these boundaries, we significantly reduce the number of cells processed at any given time.

The \textit{two\allowbreak\_dimensional\allowbreak\_scrollables} package provides interfaces where the logic for laying out the cells can be implemented. Parameters that describe the viewport, including the horizontal and vertical offsets as well as the viewport height and width, are supplied by these interfaces.

Algorithm~\ref{alg:layout_cells} outlines our algorithm to arrange the visible cells within the viewport.

\begin{algorithm} \caption{Layout of Visible Spreadsheet Cells} \label{alg:layout_cells} \begin{algorithmic}[1] \STATE \textbf{Initialize Indices} \STATE \quad $\mathit{leadingColumnIndex} \leftarrow$ column index corresponding to $\mathit{horizontalOffset}$ \STATE \quad $\mathit{leadingRowIndex} \leftarrow$ row index corresponding to $\mathit{verticalOffset}$ \STATE \quad $\mathit{trailingColumnIndex} \leftarrow$ column index corresponding to $\mathit{horizontalOffset} + \mathit{viewportWidth}$ \STATE \quad $\mathit{trailingRowIndex} \leftarrow$ row index corresponding to $\mathit{verticalOffset} + \mathit{viewportHeight}$

\STATE \textbf{Calculate Initial Offsets} \STATE \quad $\mathit{leadingColumnOffset} \leftarrow$ sum of widths from the first column up to $\mathit{leadingColumnIndex}$ \STATE \quad $\mathit{leadingRowOffset} \leftarrow$ sum of heights from the first row up to $\mathit{leadingRowIndex}$ \STATE \quad $\mathit{horizontalLayoutOffset} \leftarrow \mathit{leadingColumnOffset} - \mathit{horizontalOffset}$

\FOR{each $\mathit{columnIndex}$ from $\mathit{leadingColumnIndex}$ to $\mathit{trailingColumnIndex}$} \STATE \quad $\mathit{verticalLayoutOffset} \leftarrow \mathit{leadingRowOffset} - \mathit{verticalOffset}$

    
    \FOR{each $\mathit{rowIndex}$ from $\mathit{leadingRowIndex}$ to $\mathit{trailingRowIndex}$}
        \STATE \quad $\mathit{cell} \leftarrow$ build or retrieve the cell at ($\mathit{columnIndex}$, $\mathit{rowIndex}$)
        \STATE \quad Layout $\mathit{cell}$ at position ($\mathit{horizontalLayoutOffset}$, $\mathit{verticalLayoutOffset}$)
    
        \IF{a custom height is defined for row $\mathit{rowIndex}$}
            \STATE \quad $\mathit{verticalLayoutOffset} \leftarrow \mathit{verticalLayoutOffset} +$ height of row $\mathit{rowIndex}$
        \ELSE
            \STATE \quad $\mathit{verticalLayoutOffset} \leftarrow \mathit{verticalLayoutOffset} + \mathit{defaultRowHeight}$
        \ENDIF
    \ENDFOR
    \STATE \quad $\mathit{columnWidth} \leftarrow$ width of column $\mathit{columnIndex}$
    \STATE \quad $\mathit{horizontalLayoutOffset} \leftarrow \mathit{horizontalLayoutOffset} + \mathit{columnWidth}$
\ENDFOR \end{algorithmic} \end{algorithm}

By applying this method, we render only the cells necessary for the current view, thereby optimizing performance and ensuring smooth user interactions even with large and complex worksheets.







 


 



\section{Evaluation}

The fundamental approach of the Spreadsheet Data Extractor, based on the converter by Aue et al.~\cite{alexander2024converting}, upon which we build, remains unchanged. The effectiveness of this approach has already been investigated. Aue et al. evaluated the extraction of data from over 500 Excel files. The time required for each file was determined from a sample of 331 processed Excel files comprising 3,093 worksheets. On average, student assistants needed 15 minutes per file and 95 seconds per worksheet.

Our focus is on improving the user experience and optimizing the performance of the Spreadsheet Data Extractor. We enhanced the user experience by displaying the Excel worksheets similarly to how they appear in Excel and by reducing the number of required user interactions through the integration of the selection hierarchy, worksheet view, and output preview into a single interface. Performance was further improved by implementing incremental loading of Excel files and rendering only the visible cells.

\subsection{Acceleration When Opening Files}

To evaluate the performance improvements when opening Excel files, we conducted a series of tests using a large Excel file. We downloaded the entire collection of Excel files from the German Federal Statistical Office (Destatis)\footnote{\url{https://www.destatis.de}}, which provides extensive statistical data across various domains. From this collection, we identified the largest file~\cite{destatis_rechnungsergebnis_2024}, which has a compressed size of 87~MB and an uncompressed size of 911~MB.

We performed three sets of tests to compare the performance of different methods for opening and reading data from this Excel file:
 
\begin{enumerate}
    \item \textbf{VBA Script in Excel}: We wrote a VBA script that opens the Excel file and reads specific values from a worksheet. This script simulates how users might interact with the file using Excel's built-in capabilities.
    
    \item \textbf{Spreadsheet Data Extractor}: We developed an equivalent unit test using the functions implemented in our Spreadsheet Data Extractor to open the same file and read the same cells. This test aimed to assess the performance of our tool under the same conditions.
    
    \item \textbf{\texttt{excel} Package}: We tested the \texttt{excel} package \cite{excelPackage} used by Aue et al.~\cite{alexander2024converting}, creating a unit test that attempts to read the same cells from the file. This test was intended to benchmark the performance of the prior solution.
\end{enumerate}

All tests were conducted on a machine with the following specifications: Intel Core i5-10210U quad-core CPU at 1.60~GHz, 16~GB RAM, and a solid-state drive (SSD), running Windows 10 Pro. The version of Microsoft Excel used was Microsoft Office LTSC Professional Plus 2021.

We ran the script 10 times, measuring the time required to open the file and read the values in each iteration. The results of these runtime measurements are presented in Figure~\ref{fig:boxplot_run_times_to_open_worksheet}. 
The Spreadsheet Data Extractor opened the worksheet with a median time of 120 milliseconds and an average time of 178 milliseconds. The first run took 668~milliseconds, possibly because the file was not yet cached and had to be loaded from the disk.
In contrast, Excel opened the worksheet with a median time of 40.281~seconds and an average time of 41.138~seconds.
The Dart \texttt{excel} package~\cite{excelPackage} used in the previous work took 13 minutes and 15 seconds to open the worksheet in the first run. The other nine runs could not be completed because an out-of-memory exception was thrown during the second run.

These results demonstrate that the Spreadsheet Data Extractor opens worksheets over two orders of magnitude faster than Excel and nearly four orders of magnitude faster than the Dart \texttt{excel} package used in prior work.

\begin{figure}[h] 
  \centering 
  \includegraphics[width=\linewidth]{charts/boxplot_run_times_to_open_worksheet.pdf} 
  \caption{Boxplot of Worksheet Opening Times Comparing the Spreadsheet Data Extractor, Microsoft Excel, and the Dart \texttt{excel} Package} 
  \label{fig:boxplot_run_times_to_open_worksheet} 
\end{figure}

\section{Future Work}

We plan to continue improving the Spreadsheet Data Extractor by implementing new features that enhance the user experience and address current limitations.
%One such improvement is the correct rendering of text aligned using Excel's "Center Across Selection" horizontal alignment.
In parallel, we intend to test the tool on additional datasets to further evaluate its effectiveness and efficiency.

%\subsection{Implementing "Center Across Selection"}
%
%Although we can analyze cell merging—a commonly used feature in Excel to center text across multiple cells—this worksheet does not use this visual formatting feature through cell merging. Instead, the first cell has a horizontal alignment set to \texttt{centerContinuous}. Our previous attempts to determine the cell coordinates over which this horizontal centering extends have been unsuccessful. Further testing and research are necessary to understand how such centered texts are stored in the XML structure of the Excel file.
%
%\begin{figure}[h] 
%  \centering 
%  \includegraphics[width=\linewidth]{images/spreadsheet_data_extractor/new_version/center.png} 
%  \caption{Illustration of the Text Alignment "Center Across Selection" in Excel} 
%  \label{fig:center} 
%\end{figure}
%
%\subsection{Evaluation on Real-World Data}

To date, the base version of the tool has been tested on a dataset from the Agricultural Structure Survey on land use and livestock in Germany for 2020. It would be valuable to test the new version of the tool on data from before 2020 to assess whether the improvements we have made enhance the tool's effectiveness.

We plan to utilize the timestamps documented in the output CSV files to compare them with new timestamps obtained from re-testing. This comparison will help us determine whether the student assistants can work faster with the new version of the tool.

\section{Conclusion}

In this paper, we introduced the Spreadsheet Data Extractor (SDE)~\cite{spreadsheet_data_extractor}, an enhanced tool that builds upon the foundational work of Alexander Aue et al.~\cite{alexander2024converting}. By addressing key limitations of the existing solution, we implemented significant performance optimizations and usability enhancements. Specifically, SDE employs incremental loading of worksheets and optimizes rendering by processing only the visible cells, resulting in performance improvements that enable the tool to open large Excel files.

We also integrated the selection hierarchy, worksheet view, and output preview into a unified interface, streamlining the data extraction process.By adopting a user-centric approach that gives users full control over data selection and metadata hierarchy definition without requiring programming knowledge, we provide a robust and accessible solution for data extraction. Our tool offers user-friendly features such as the ability to duplicate hierarchies of columns and tables and to move them over similar structures for reuse, reducing the need for repetitive configurations.

By combining the strengths of the original approach with our enhancements in user interface and performance optimizations, our tool significantly improves the efficiency and reliability of data extraction from diverse and complex spreadsheet formats.
  

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput















%\subsubsection{The User Needs to Know Beforehand When to Create Empty Nodes}




 
  
%\subsubsection{Support for Multi-Node Selection}
%If the user wishes to freeze cells before duplicating and moving a hierarchy,
%the user should be able to select these cells by clicking directly on them.
%This could be sped up by allowing the user to select multiple sets of cells
%by drawing a line around them,
%as is done in other GUIs and is often referred to as the lasso tool.

%\subsubsection{Drag-and-Drop Node Manipulation}
%Enabling users to drag and drop nodes to rearrange them
%within the hierarchy would simplify the process
%of organizing and structuring data,
%providing a more intuitive way to describe the hierarchy and 
%allowing for quick corrections in case of errors.

%\subsubsection{Node Wrapping Functionality}

%Introducing the feature to wrap user-selected nodes
%into new ones would provide users with the flexibility
%to create empty nodes on-the-fly as they recognize the need for them.
%For example, if a user adds a new column
%with more column headers than the existing ones,
%they could seamlessly wrap the previous column headers into new nodes
%to match the number of headers in each column. 

%\subsection{Enhancements for Data Export}


%While the extraction of the more than 500 Excel files was a success, it resulted in a few hundred CSV files.
%All of these files are in a machine-readable format but are split into many separate files.
%A lot of those files have the same amount columns and the  same or a very similar set of metadata columns.
%Those files should be combined, because they contain the same type of data, yet different values.
%The reason for this is, because the Spreadsheet Data Extractor made it easy for the user to extract data from just one or a few of Excel files.
%Using it for a lot of files was impractical, because the task view grew very big with just extracting from one file.
%To maintain an overview the student assistants instead created a new configuration when moving to the next Excel file.
%That means that the extracted files needed to be combined afterward by comparing them with each other.
%When two files are similar, they needed to be merged into a single file
